---
title: Semantic Search App
emoji: ğŸ“„ğŸ”—ğŸ§ â“ğŸ”—ğŸ¤–
colorFrom: indigo
colorTo: pink
sdk: gradio
sdk_version: 5.46.1
app_file: app/app.py
pinned: false
---

# Semantic Search App (ğŸ“„ â†’ ğŸ”— â†’ ğŸ§  â†’ â“ â†’ ğŸ”— â†’ ğŸ¤–)

Upload a PDF, ask questions, and get context-aware answers powered by LangChain, ChromaDB, and NVIDIA/Google LLMs â€” all wrapped in a clean Gradio interface.

ğŸ”— **Live Demo**: [https://huggingface.co/spaces/frkhan/semantic-search-app](#)

---

### ğŸš€ Features

- ğŸ“„ Upload and process PDF documents  
- ğŸ” Perform semantic search using vector embeddings  
- ğŸ¤– Get answers from powerful LLMs (NVIDIA or Google Gemini)  
- ğŸ§  Uses LangChain + ChromaDB for retrieval  
- ğŸ§° Docker-ready and Hugging Face Spacesâ€“compatible  

---

### ğŸ› ï¸ Tech Stack

| Component        | Purpose                                 |
|------------------|-----------------------------------------|
| LangChain        | Orchestration of embedding + LLM calls  |
| ChromaDB         | Vector database for semantic retrieval  |
| NVIDIA / Gemini  | Embedding + LLM APIs                    |
| Gradio           | Interactive UI                          |
| Docker           | Containerized deployment                |

---

## ğŸ“¦ Installation

### Option 1: Run Locally

```bash
git clone https://github.com/KI-IAN/semantic-search.git
cd semantic-search
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```


Create a .env file in the root directory:

```env
GOOGLE_API_KEY=your_google_api_key
NVIDIA_API_KEY=your_nvidia_api_key
CHROMA_DIR=./chroma_db
```

Then run:

```bash
python app/app.py
```

---

### Option 2: Run with Docker

```bash
docker-compose up --build
```

Access the app at http://localhost:1200

---

### Option 3: Deploy on Hugging Face Spaces

Create a new Space â†’ choose Gradio as the SDK

Upload your project files (including app/, Dockerfile, requirements.txt, .env)

Set Secrets in the â€œSecretsâ€ tab:

GOOGLE_API_KEY

NVIDIA_API_KEY

(Optional) CHROMA_DIR (defaults to ./chroma_db)

Hugging Face will auto-detect and launch the app via Gradio

---

## ğŸ”‘ Getting API Keys

To use this app, you'll need API keys for both **Gemini** and **NVIDIA NIM**. Here's how to obtain them:

### ğŸŒ Gemini API Key
Gemini is Google's family of generative AI models. To get an API key:

1. Visit the [Google AI Studio](https://aistudio.google.com/api-keys).
2. Sign in with your Google account.
3. Click **"Create API Key"** and copy the key shown.
4. Use this key in your `.env` file or configuration as `GEMINI_API_KEY`.

> Note: Gemini API access may be limited based on region or account eligibility. Check the Gemini API [Rate Limits here](https://ai.google.dev/gemini-api/docs/rate-limits)

### ğŸš€ NVIDIA NIM API Key
NIM (NVIDIA Inference Microservices) provides hosted models via REST APIs. To get started:

1. Go to the [NVIDIA API Catalog](https://build.nvidia.com/?integrate_nim=true&hosted_api=true&modal=integrate-nim).
2. Choose a model (e.g., `nim-gemma`, `nim-mistral`, etc.) and click **"Get API Key"**.
3. Sign in or create an NVIDIA account if prompted.
4. Copy your key and use it as `NVIDIA_NIM_API_KEY` in your environment.

> Tip: You can test NIM endpoints directly in the browser before integrating.

---

Once you have both keys, store them securely and never commit them to version control.

---


### ğŸ§ª How to Use
Upload a PDF â€” drag and drop your document

Click â€œğŸ“„ Process Documentâ€ â€” the app will split, embed, and store the content

Enter a query â€” ask a question like:

â€œWhat are the key findings?â€

â€œSummarize the methodology.â€

â€œWhat does the report say about climate change?â€

Click â€œğŸ” Ask a Questionâ€ â€” get semantic search results and an LLM-generated answer

---

### âš™ï¸ Configuration
All secrets are loaded from .env or Hugging Face Secrets tab:

| Variable        | Description                                 |
|------------------|-----------------------------------------|
| GOOGLE_API_KEY        | Gemini LLM API key  |
| NVIDIA_API_KEY         | NVIDIA LLM API key  |
| CHROMA_DIR  | Path to store Chroma vector DB                    |

---

### ğŸ§© Customization

Switch between NVIDIA and Gemini embeddings in process_pdf()

Change LLM model in search_query() (bytedance/seed-oss-36b-instruct, gemini-2.5-pro, etc.)

Tune chunk size and overlap in RecursiveCharacterTextSplitter

Add dropdowns to UI for model selection (optional)

---

### ğŸ“ File Structure

```Code
semantic-search/
â”œâ”€â”€ .env
â”œâ”€â”€ .github/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ app/
    â”œâ”€â”€ app.py
    â””â”€â”€ config.py
```

---

## ğŸ“œ License

This project is open-source and distributed under the **[MIT License](https://opensource.org/licenses/MIT)**. Feel free to use, modify, and distribute it with attribution.

---


## ğŸ¤ Acknowledgements

- [LangChain](https://www.langchain.com) â€” Powerful framework for orchestrating LLMs, embeddings, and retrieval pipelines.
- [ChromaDB](https://www.trychroma.com/) â€” Fast and flexible open-source vector database for semantic search.
- [NVIDIA AI Endpoints](https://build.nvidia.com/models) â€” Hosted LLM and embedding APIs including Seed OSS and NV-Embed.
- [Google Gemini](https://aistudio.google.com/welcome) â€” Robust multimodal LLM platform offering text embeddings and chat models.
- [Gradio](https://www.gradio.app) â€” Simple and elegant Python library for building machine learning interfaces.
- [PyMuPDF](https://pymupdf.readthedocs.io) â€” Lightweight PDF parser for fast and accurate text extraction.
- [Docker](https://www.docker.com) â€” Containerization platform for reproducible deployment across environments.
- [Hugging Face Spaces](https://huggingface.co/spaces) â€” Free hosting platform for ML demos with secret management and GPU support.
